# -*- coding: utf-8 -*-
"""RNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bsVnr_gXqB-mhWLzmkkuHtbeqAZIyEOv
"""

!rm -rf sample_data
!mkdir ./data
!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1dPHIl8ZnfDz_fxNd2ZeBYedTat2lfxcO' -O './data/training_label.txt'
!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1x1rJOX_ETqnOZjdMAbEE2pqIjRNa8xcc' -O './data/training_nolabel.txt'
!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=16CtnQwSDCob9xmm6EdHHR7PNFNiOrQ30' -O './data/testing_data.txt'

!ls data/

import os
import argparse
import numpy as np
import pandas as pd
import argparse
import torch

import torch.optim as optim
import torch.nn.functional as F

from torch import nn
from torch.utils import data
from gensim.models import word2vec
from sklearn.model_selection import train_test_split

def load_train_label_data(path):
  with open(path, 'r', encoding="utf-8") as f:
      lines = f.readlines()
      lines = [line.strip('\n').split(' ') for line in lines]
      train_x = [line[2:] for line in lines]
      train_y = [line[0] for line in lines]
  return train_x, train_y


def load_train_nolabel_data(path):
  with open(path, 'r', encoding="utf-8") as f:
      lines = f.readlines()
      train_x = [line.strip('\n').split(' ') for line in lines]
  return train_x


def load_testing_data(path):
  with open(path, 'r',encoding="utf-8") as f:
      lines = f.readlines()
      test_x = ["".join(line.strip('\n').split(",")[1:]).strip() for line in lines[1:]]
      test_x = [sen.split(' ') for sen in X]
  return test_x


if __name__ == '__main__':
  train_x, train_y = load_train_label_data('./data/training_label.txt')
  train_nolabel_x = load_train_nolabel_data('./data/training_label.txt')
  test_x = load_train_nolabel_data('./data/testing_data.txt')

def train_word2vec(x):
  # 訓練 word to vector 的 word embedding
  model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)
  return model

from google.colab import drive
drive.mount('/content/drive')

!ls drive/MyDrive/app/rnn

#model = train_word2vec(train_x + train_nolabel_x + test_x)
#model.save(w2v_path)

model_path='drive/MyDrive/app/rnn'
w2v_path = os.path.join(model_path, 'w2v_all.model')
sen_len = 20
batch_size = 1024
epoch = 500

model = word2vec.Word2Vec.load(w2v_path)
print(model)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

from gensim.models import Word2Vec

class Preprocess():
  def __init__(self, sentences, sen_len, w2v_path):
    self.w2v_path = w2v_path
    self.sentences = sentences
    self.sen_len = sen_len
    self.idx2word = []
    self.word2idx = {}
    self.embedding_matrix = []
  
  def get_w2v_model(self):
    self.embedding = Word2Vec.load(self.w2v_path)
    self.embedding_dim = self.embedding.vector_size

  def add_embedding(self, word):
    # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector
    # word 只會是 "<PAD>" 或 "<UNK>"
    vector = torch.empty(1, self.embedding_dim)
    torch.nn.init.uniform_(vector)
    self.word2idx[word] = len(self.word2idx)
    self.idx2word.append(word)
    self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)
  
  def make_embedding(self,):
    self.get_w2v_model()
    for i, word in enumerate(self.embedding.wv.vocab):
      #e.g. self.embedding_matrix[1] = 'he' vector
      self.word2idx[word] = i#dictionary
      self.idx2word.append(word)#list
      self.embedding_matrix.append(self.embedding[word])#list
    self.embedding_matrix = torch.tensor(self.embedding_matrix)
    self.add_embedding("<PAD>")
    self.add_embedding("<UNK>")
    return self.embedding_matrix#31565

  def pad_sequence(self, sentence):
    if len(sentence) > self.sen_len:
      sentence = sentence[:self.sen_len]
    else:
      pad_len = self.sen_len - len(sentence)
      sentence = [self.word2idx["<PAD>"]] * pad_len + sentence
    assert len(sentence) == self.sen_len
    return sentence

  def sentence_word2idx(self):
    # 把句子裡面的字轉成相對應的 index
    sentence_list = []
    for i, sen in enumerate(self.sentences):
      sentence_idx = []
      for word in sen:
        if (word in self.word2idx.keys()):
          sentence_idx.append(self.word2idx[word])
        else:
          sentence_idx.append(self.word2idx["<UNK>"])
      sentence_idx = self.pad_sequence(sentence_idx)
      sentence_list.append(sentence_idx)
    return torch.LongTensor(sentence_list)

  def labels_to_tensor(self, y):
    y = [int(label) for label in y]
    return torch.LongTensor(y)

preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)#sen_len=20
embedding = preprocess.make_embedding()
train_x = preprocess.sentence_word2idx()
train_y = preprocess.labels_to_tensor(train_y)

train_x, validation_x, train_y, validation_y = train_x[:180000], train_x[180000:], train_y[:180000], train_y[180000:]

class SentenceDataset(data.Dataset):
  def __init__(self, data, label):
    self.data = data
    self.label = label

  def __getitem__(self, idx):
    if self.label is None:return self.data[idx]
    return self.data[idx], self.label[idx]

  def __len__(self):
    return len(self.data)

train_dataset = SentenceDataset(train_x, train_y)
val_dataset = SentenceDataset(validation_x, validation_y)

train_loader = data.DataLoader(dataset = train_dataset,batch_size = batch_size,shuffle = True,num_workers=2)
val_loader = data.DataLoader(dataset = val_dataset,batch_size = batch_size,shuffle = False,num_workers=2)

class NN_LSTM(nn.Module):
  def __init__(self, embedding, fix_embedding=True):
    super(NN_LSTM, self).__init__()
    self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))#31563 250 #词汇表
    self.embedding.weight = torch.nn.Parameter(embedding)
    self.embedding.weight.requires_grad = False

    self.lstm = nn.LSTM(input_size=250,hidden_size=150,num_layers=1, batch_first=True)#num_layers 层数 hidden_size一层的神经元个数
    
    self.classifier = nn.Sequential(
        nn.Linear(150, 50),
        nn.Dropout(0.5),
        nn.Linear(50, 1),
        nn.Dropout(0.5),
        nn.Sigmoid()
    )

  def forward(self, inputs):#inputs [128 20]
    inputs = self.embedding(inputs)#output  128 20 250 lstm (batch, seq_len, input_size)#batch_first
    x, _ = self.lstm(inputs, None)#隐藏层大小 [128 20 150]  (batch, seq_len, hidden_size)
    x = x[:, -1, :]#取用 LSTM 最後一層的 hidden state [128 150]
    y = self.classifier(x)
    return y

# 製作一個 model 的對象
model = NN_LSTM(embedding, fix_embedding=True)
model = model.to(device)

def evaluation(outputs, labels):
  outputs[outputs>=0.5] = 1 # 大於等於 0.5 為正面
  outputs[outputs<0.5] = 0 # 小於 0.5 為負面
  correct = torch.sum(torch.eq(outputs, labels)).item()
  return correct

model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數
criterion = nn.BCELoss() # 定義損失函數，這裡我們使用 binary cross entropy loss
t_batch = len(train_loader)#8000/128
v_batch = len(val_loader)#2000/128
optimizer = optim.Adam(model.parameters(), lr=0.001)
best_acc = 0, 0, 0
for epoch_num in range(epoch):
  total_loss, total_acc = 0, 0
  for i, (inputs, labels) in enumerate(train_loader):
    inputs = inputs.to(device, dtype=torch.long)#[128 20]
    labels = labels.to(device, dtype=torch.float)

    optimizer.zero_grad() #loss.backward()的gradient會累加，所以每次餵完一個 batch 後需要歸零
    outputs = model(inputs) #[128 1]
    outputs = outputs.squeeze()#[128]
    loss = criterion(outputs, labels) # 計算此時模型的 training loss
    loss.backward() # 算 loss 的 gradient

    optimizer.step() # 更新訓練模型的參數
    
    correct = evaluation(outputs, labels) # 計算此時模型的 training accuracy
    total_acc = total_acc + (correct / batch_size)
    total_loss = total_loss +  loss.item()
    #print(f'[ Epoch:[{epoch_num+1:0=3d}/{epoch:0=3d}] [{i+1:0=4d}/{t_batch}] loss:{loss.item():.3f} acc:{correct*100/batch_size:.3f}',end='\r')
  print(f'\nTrain | Loss:{total_loss/t_batch:.5f} Acc: {total_acc/t_batch*100:.3f}')

  model.eval()
  with torch.no_grad():
    total_loss, total_acc = 0, 0
    for i, (inputs, labels) in enumerate(val_loader):
      inputs = inputs.to(device, dtype=torch.long) # device 為 "cuda"，將 inputs 轉成 torch.cuda.LongTensor
      labels = labels.to(device, dtype=torch.float) # device 為 "cuda"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float
      outputs = model(inputs) # 將 input 餵給模型
      outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()
      loss = criterion(outputs, labels) # 計算此時模型的 validation loss
      correct = evaluation(outputs, labels) # 計算此時模型的 validation accuracy
      total_acc += (correct / batch_size)
      total_loss += loss.item()

    print("Valid | Loss:{:.5f} Acc: {:.3f} ".format(total_loss/v_batch, total_acc/v_batch*100))
  model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數（因為剛剛轉成 eval 模式）