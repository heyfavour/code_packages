# -*- coding: utf-8 -*-
"""explain_model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BDoUfQutud-s8nUqRGqjkne-VFbBTOan
"""

# 下載並解壓縮訓練資料
!gdown --id '19CzXudqN58R3D-1G8KeFWk8UDQwlb8is' --output food-11.zip
!unzip food-11.zip
!mv food-11 data

# 下載 pretrained model，這裡是用助教的 model demo，寫作業時要換成自己的 model
!gdown --id '1CShZHsO8oAZwxQkMe7jRtEgSNb2w_OZu' --output checkpoint.pth

!pip install lime

import os
import sys
import torch
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import torchvision.transforms as transforms


from PIL import Image
from torch.optim import Adam
from torch.utils.data import Dataset

# 這是助教的示範 model，寫作業時一樣要換成自己的
class Classifier(nn.Module):
  def __init__(self):
    super(Classifier, self).__init__()
    def building_block(indim, outdim):
      return [
        nn.Conv2d(indim, outdim, 3, 1, 1),
        nn.BatchNorm2d(outdim),
        nn.ReLU(),
      ]
    def stack_blocks(indim, outdim, block_num):
      layers = building_block(indim, outdim)
      for i in range(block_num - 1):
        layers += building_block(outdim, outdim)
      layers.append(nn.MaxPool2d(2, 2, 0))
      return layers

    cnn_list = []
    cnn_list += stack_blocks(3, 128, 3)
    cnn_list += stack_blocks(128, 128, 3)
    cnn_list += stack_blocks(128, 256, 3)
    cnn_list += stack_blocks(256, 512, 1)
    cnn_list += stack_blocks(512, 512, 1)
    self.cnn = nn.Sequential(*cnn_list)

    dnn_list = [
      nn.Linear(512 * 4 * 4, 1024),
      nn.ReLU(),
      nn.Dropout(p = 0.3),
      nn.Linear(1024, 11),
    ]
    self.fc = nn.Sequential(*dnn_list)

  def forward(self, x):
    out = self.cnn(x)
    out = out.reshape(out.size()[0], -1)
    return self.fc(out)

model = Classifier().cuda()
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])

def get_paths_labels(path):
  imgnames = os.listdir(path)
  imgnames.sort()
  imgpaths = []
  labels = []
  for name in imgnames:
    imgpaths.append(os.path.join(path, name))
    labels.append(int(name.split('_')[0]))
  return imgpaths, labels
train_paths, train_labels = get_paths_labels('./data/training')

# 助教 training 時定義的 dataset
# 因為 training 的時候助教有使用底下那些 transforms，所以 testing 時也要讓 test data 使用同樣的 transform
# dataset 這部分的 code 基本上不應該出現在你的作業裡，你應該使用自己當初 train HW3 時的 preprocessing
class FoodDataset(Dataset):
  def __init__(self, paths, labels, mode):
    # mode: 'train' or 'eval'
    self.paths = paths
    self.labels = labels
    trainTransform = transforms.Compose([
      transforms.Resize(size=(128, 128)),
      transforms.RandomHorizontalFlip(),
      transforms.RandomRotation(15),
      transforms.ToTensor(),
    ])
    evalTransform = transforms.Compose([
      transforms.Resize(size=(128, 128)),
      transforms.ToTensor(),
    ])
    self.transform = trainTransform if mode == 'train' else evalTransform

  # 這個 FoodDataset 繼承了 pytorch 的 Dataset class
  # 而 __len__ 和 __getitem__ 是定義一個 pytorch dataset 時一定要 implement 的兩個 methods
  def __len__(self):
    return len(self.paths)

  def __getitem__(self, index):
    X = Image.open(self.paths[index])
    X = self.transform(X)
    Y = self.labels[index]
    return X, Y

  # 這個 method 並不是 pytorch dataset 必要，只是方便未來我們想要指定「取哪幾張圖片」出來當作一個 batch 來 visualize
  def getbatch(self, indices):
    images = []
    labels = []
    for index in indices:
      image, label = self.__getitem__(index)
      images.append(image)
      labels.append(label)
    return torch.stack(images), torch.tensor(labels)
train_set = FoodDataset(train_paths, train_labels, mode='eval')
img_indices = [1, 2, 3, 4]#想要查看的图片
images, labels = train_set.getbatch(img_indices)#得到想要查看的数据

def normalize(image):
  return (image - image.min()) / (image.max() - image.min())

def compute_saliency_maps(x, y, model):
  model.eval()
  x = x.cuda()#[4,3,128,128]

  # 最關鍵的一行 code
  # 因為我們要計算 loss 對 input image 的微分，原本 input x 只是一個 tensor，預設不需要 gradient
  # 這邊我們明確的告知 pytorch 這個 input x 需要gradient，這樣我們執行 backward 後 x.grad 才會有微分的值
  x.requires_grad_()
  
  y_pred = model(x)#[4,11]

  loss_func = torch.nn.CrossEntropyLoss()
  loss = loss_func(y_pred, y.cuda())
  loss.backward()

  saliencies = x.grad.abs().detach().cpu()#[4,3,128,128]
  # saliencies: (batches, channels, height, weight)
  # 因為接下來我們要對每張圖片畫 saliency map，每張圖片的 gradient scale 很可能有巨大落差
  # 可能第一張圖片的 gradient 在 100 ~ 1000，但第二張圖片的 gradient 在 0.001 ~ 0.0001
  # 如果我們用同樣的色階去畫每一張 saliency 的話，第一張可能就全部都很亮，第二張就全部都很暗，
  # 如此就看不到有意義的結果，我們想看的是「單一張 saliency 內部的大小關係」，
  # 所以這邊我們要對每張 saliency 各自做 normalize。手法有很多種，這邊只採用最簡單的
  saliencies = torch.stack([normalize(item) for item in saliencies])
  return saliencies
saliencies = compute_saliency_maps(images, labels, model)

# 使用 matplotlib 畫出來
fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))
for row, target in enumerate([images, saliencies]):
  for column, img in enumerate(target):
    #permute tensor (channels, height, width)=> matplolib  (height, width, channels)
    axs[row][column].imshow(img.permute(1, 2, 0).numpy())
 
plt.show()
plt.close()
# 從第二張圖片的 saliency，我們可以發現 model 有認出蛋黃的位置
# 從第三、四張圖片的 saliency，雖然不知道 model 細部用食物的哪個位置判斷，但可以發現 model 找出了食物的大致輪廓



from skimage.segmentation import slic
from lime import lime_image

def predict(input):
    # input: numpy array, (batches, height, width, channels)                                                                                                                                                     
    model.eval()                                                                                                                                                             
    input = torch.FloatTensor(input).permute(0, 3, 1, 2)                                                                                                          
    # 需要先將 input 轉成 pytorch tensor，且符合 pytorch 習慣的 dimension 定義
    # 也就是 (batches, channels, height, width)
    output = model(input.cuda())                                                                                                                                             
    return output.detach().cpu().numpy()                                                                                                                              
                                                                                                                                                                             
def segmentation(input):
    # 利用 skimage 提供的 segmentation 將圖片分成 300 塊                                                                                                                                      
    return slic(input, n_segments=200, compactness=1, sigma=1)  
fig, axs = plt.subplots(1, 4, figsize=(15, 8))                                                                                                                                                                 


for idx, (image, label) in enumerate(zip(images.permute(0, 2, 3, 1).numpy(), labels)):#matplolib  (height, width, channels)                                                                                                                                            
    explainer = lime_image.LimeImageExplainer()  
    # classifier_fn 定義圖片如何經過 model 得到 prediction
    # segmentation_fn 定義如何把圖片做 segmentation                                                                                                                            
    explaination = explainer.explain_instance(image=image.astype(np.double), classifier_fn=predict, segmentation_fn=segmentation)
    # doc: https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=get_image_and_mask#lime.lime_image.ImageExplanation.get_image_and_mask
    lime_img, mask = explaination.get_image_and_mask(                                                               
        label=label.item(),                                                                                                                           
        positive_only=False,                                                                                                                         
        hide_rest=False,                                                                                                                             
        num_features=11,                                                                                                                              
        min_weight=0.05                                                                                                                              
    )
    axs[idx].imshow(lime_img)

plt.show()
plt.close()