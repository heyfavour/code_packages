# -*- coding: utf-8 -*-
"""anomaly_detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DE24Fh6a-PUDTcsEmCfWZu0XE1jkbkyF
"""

!gdown --id '1_zT3JOpvXFGr7mkxs3XJDeGxTn_8pItq' --output train.npy 
!gdown --id '11Y_6JDjlhIY-M5-jW1rLRshDMqeKi9Kr' --output test.npy 
!ls

import numpy as np

train = np.load('train.npy', allow_pickle=True)#[40000 32 32 3]
test = np.load('test.npy', allow_pickle=True)#[10000 32 32 3]

"""KMeans"""

from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import f1_score, pairwise_distances, roc_auc_score
from scipy.cluster.vq import vq, kmeans

x = train.reshape(len(train), -1)#[40000 32 32 3]=>[40000 32*32*3]
y = test.reshape(len(test), -1)#[10000 32 32 3]=>[10000 32*32*3]
scores = list()
for n in range(1, 10):
  kmeans_x = MiniBatchKMeans(n_clusters=n, batch_size=100).fit(x)#model
  y_cluster = kmeans_x.predict(y)#[10000] 
  #kmeans_x.cluster_centers_.shape [n 32*32*3]
  #kmeans_x.cluster_centers_[y_cluster] [10000 32*32*3]#按预测的数据找到每个数据的质心
  y_dist = np.sum(np.square(kmeans_x.cluster_centers_[y_cluster] - y), axis=1)
  y_pred = y_dist
  #评分 此处没有y_label
  # score = f1_score(y_label, y_pred, average='micro')
  # score = roc_auc_score(y_label, y_pred, average='micro')

"""PCA"""

from sklearn.decomposition import PCA

pca = PCA(n_components=2).fit(x)#model
y_projected = pca.transform(y)#[10000 2]
y_reconstructed = pca.inverse_transform(y_projected)#[10000 32*32*3]
dist = np.sqrt(np.sum(np.square(y_reconstructed - y).reshape(len(y), -1), axis=1))

y_pred = dist
# score = roc_auc_score(y_label, y_pred, average='micro')
# score = f1_score(y_label, y_pred, average='micro')
# print('auc score: {}'.format(score))

import torch
from torch import nn
import torch.nn.functional as F

from torch.autograd import Variable
from torch.utils.data import DataLoader
from torch.optim import Adam, AdamW
from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)

"""Auto-encoder

auto-encoder FCN
"""

class nn_autoencoder(nn.Module):
  def __init__(self):
    super(nn_autoencoder, self).__init__()
    self.encoder = nn.Sequential(
      nn.Linear(32 * 32 * 3, 128),
      nn.ReLU(True),
      nn.Linear(128, 64),
      nn.ReLU(True), 
      nn.Linear(64, 12), 
      nn.ReLU(True), 
      nn.Linear(12, 3)
    )
    self.decoder = nn.Sequential(
      nn.Linear(3, 12),
      nn.ReLU(True),
      nn.Linear(12, 64),
      nn.ReLU(True),
      nn.Linear(64, 128),
      nn.ReLU(True), 
      nn.Linear(128, 32 * 32 * 3), 
      nn.Tanh()
    )

  def forward(self, x):
    code = self.encoder(x)
    ouput = self.decoder(code)
    return ouput

num_epochs = 1000
batch_size = 128
learning_rate = 1e-3

train_x = torch.tensor(train.reshape(len(train), -1), dtype=torch.float)
train_dataset = TensorDataset(train_x)
train_sampler = RandomSampler(train_dataset)#shuffle=True 自动调用
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)

model = nn_autoencoder().cuda()
criterion = nn.MSELoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

best_loss = np.inf#无穷大
model.train()
for epoch in range(num_epochs):
  for data in train_dataloader:
    img = data[0].cuda()#[128 32*32*3]
    # ===================forward=====================
    output = model(img)
    loss = criterion(output, img)
    # ===================backward====================
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  #===================log========================
  print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))

"""auto-encoder CONV"""

class conv_autoencoder(nn.Module):
  def __init__(self):
    super(conv_autoencoder, self).__init__()
    self.encoder = nn.Sequential(
      nn.Conv2d(3, 12, 4, stride=2, padding=1),#[batch, 12, 16, 16]
      nn.ReLU(),
      nn.Conv2d(12, 24, 4, stride=2, padding=1),#[batch, 24, 8, 8]
      nn.ReLU(),
      nn.Conv2d(24, 48, 4, stride=2, padding=1),#[batch, 48, 4, 4]
      nn.ReLU(),
      #nn.Conv2d(48, 96, 4, stride=2, padding=1),#[batch, 96, 2, 2]
      #nn.ReLU(),
    )
    self.decoder = nn.Sequential(
      #nn.ConvTranspose2d(96, 48, 4, stride=2, padding=1),#[batch, 48, 4, 4]
      #nn.ReLU(),
      nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),#[batch, 24, 8, 8]
      nn.ReLU(),
      nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),#[batch, 12, 16, 16]
      nn.ReLU(),
      nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),#batch, 3, 32, 32]
      nn.Tanh(),
    )

  def forward(self, x):
    code = self.encoder(x)
    ouput = self.decoder(code)
    return ouput



num_epochs = 1000
batch_size = 128
learning_rate = 1e-3

train_x = torch.tensor(train, dtype=torch.float)
train_dataset = TensorDataset(train_x)
train_sampler = RandomSampler(train_dataset)#shuffle=True 自动调用
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)

model = conv_autoencoder().cuda()
criterion = nn.MSELoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

best_loss = np.inf#无穷大
model.train()
for epoch in range(num_epochs):
  for data in train_dataloader:
    img = data[0].transpose(3, 1).cuda()#[128 3 32 32]
    # ===================forward=====================
    output = model(img)
    loss = criterion(output, img)
    # ===================backward====================
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  # ===================log========================
  print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))

"""VAE"""

class VAE(nn.Module):
  def __init__(self):
    super(VAE, self).__init__()
    self.fc1 = nn.Linear(32*32*3, 400)#
    self.fc21 = nn.Linear(400, 20)#fc1=>fc21 
    self.fc22 = nn.Linear(400, 20)#fc1=>fc22
    self.fc3 = nn.Linear(20, 400)
    self.fc4 = nn.Linear(400, 32*32*3)

  def encode(self, x):
    h1 = F.relu(self.fc1(x))
    return self.fc21(h1), self.fc22(h1)
  
  def decode(self, z):
    h3 = F.relu(self.fc3(z))
    output = F.sigmoid(self.fc4(h3))
    return output

  def reparametrize(self, mu, logvar):#[20] [20]
    std = logvar.mul(0.5).exp_()#n=>e^n*0.5
    if torch.cuda.is_available():#正太分布
      eps = torch.cuda.FloatTensor(std.size()).normal_()
    else:
      eps = torch.FloatTensor(std.size()).normal_()
    eps = Variable(eps)
    return eps.mul(std).add_(mu)

  def forward(self, x):
    mu, logvar = self.encode(x)#[20] [20]
    z = self.reparametrize(mu, logvar)
    return self.decode(z), mu, logvar


def loss_vae(recon_x, x, mu, logvar, criterion):
  """
  recon_x: generating images
  x: origin images
  mu: latent mean
  logvar: latent log variance
  """
  mse = criterion(recon_x, x)  # mse loss
  # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
  KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)
  KLD = torch.sum(KLD_element).mul_(-0.5)
  # KL divergence
  return mse + KLD

num_epochs = 100
batch_size = 128
learning_rate = 1e-3

train_x = torch.tensor(train.reshape(len(train), -1), dtype=torch.float)
train_dataset = TensorDataset(train_x)  
train_sampler = RandomSampler(train_dataset)#shuffle=True 自动调用
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)


model = VAE().cuda()
criterion = nn.MSELoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

best_loss = np.inf#无穷大
model.train()
for epoch in range(num_epochs):
  for data in train_dataloader:
    img = data[0].cuda()
    # ===================forward=====================
    output = model(img)#[output code1 code2]
    loss = loss_vae(output[0], img, output[1], output[2], criterion)
    # ===================backward====================
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  # ===================log========================
  print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))